{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Copy the code from [Medium](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "class NeuralNetwork:    \n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        \n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(y.shape)\n",
    "    \n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "        \n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01875185]\n",
      " [0.98102158]\n",
      " [0.97714535]\n",
      " [0.02271424]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "nn = NeuralNetwork(X,y)\n",
    "\n",
    "for i in range(1500):\n",
    "    nn.feedforward()\n",
    "    nn.backprop()\n",
    "\n",
    "print(nn.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details\n",
    "The neural network above has 2 fully connected layers, with sigmoid activations. For simplicity the biases of the fully connected layers were set to 0.\n",
    "\n",
    "#### Fully Connected Layer\n",
    "A fully connected layer is simply the following function, where $n,m \\in \\mathbb{N}$ are positive integers.\n",
    "\n",
    "$$ f_{W,b} (x) = Wx + b, \\quad x \\in \\mathbb{R}^n, W \\in \\mathbb{R^{m\\times n}}, b \\in \\mathbb{R}^m. $$\n",
    "\n",
    "This is the general form of an affine linear transform of $x$ from an $n$-dimensional vector space to an $m$-dimensional vector space. It is said to be fully connected because each element of the output depends of every element of the input, that is if $y = f_{W,b} (x)$ then in Einstein notation\n",
    "\n",
    "$$ y_i = W_{ij} x_j + b_i $$\n",
    "\n",
    "which implies\n",
    "\n",
    "$$ \\frac{\\partial y_i}{\\partial x_j} = W_{ij}, \\quad \\text{for all } i= 1, \\ldots, m, \\, j = 1, \\ldots, n. $$\n",
    "\n",
    "In particular $y_i$ is dependent on $x_j$ for each $i$ and $j$, unless $W_{ij} = 0$.\n",
    "\n",
    "#### Sigmoid Activation\n",
    "Activation functions are typically mappings $g:\\mathbb{R}^m \\rightarrow \\mathbb{R}^m$, that act as filters on the input values, for example by bounding the output values into a finte range. \n",
    "\n",
    "\n",
    "The sigmoid function here is defined by\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1+e^{-x}}$.\n",
    "\n",
    "Since $e^{-x} > 0$, this implies $\\sigma(x) < 1$, and $\\sigma(x)>0$ is clear. As such, a sigmoid activation bounds values to the range $(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10,10,500)\n",
    "y = 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "plt.xlabel('$x$',fontsize=14)\n",
    "plt.ylabel('$\\sigma(x)$',fontsize=14)\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,0.5*np.ones((500,1)),'r-.')\n",
    "plt.title('Sigmoid Activation', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network\n",
    "\n",
    "##### Forward\n",
    "At the most abstract level, a forward pass of a neural network is simply a sequence of functions, $f_1, \\ldots, f_\\lambda$, such that\n",
    "\n",
    "$$ \\hat{y} = f_\\lambda \\circ f_{\\lambda-1} \\circ \\ldots \\circ f_1 (x). $$\n",
    "\n",
    "Here $\\hat{y}$ denotes an \"approximator\" to $y$, since $y$ will typically denote the \"true value\" the network is trying to predict.\n",
    "\n",
    "For each $f_i$ there may be associated parameters $\\omega_{i_j}$. For example the fully connected layer $f_{W,b} = Wx + b$ had parameters $W$ and $b$. In this case these parameters are called the weights and bias respectively.\n",
    "\n",
    "The aim of \"training\" a neural network is to tune such parameters so that $\\hat{y}$ is a good approximation to $y$. A good approximation is quantified by a loss function.\n",
    "\n",
    "##### Loss Function\n",
    "\n",
    "The simplest loss function is the mean square error. Given a set of true values $\\{ y_i \\}_{i=1 ^N}$ and a set of approximations $\\{ \\hat{y}_i \\}_{i= 1^N}$ the mean square error is simply\n",
    "\n",
    "$$ L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N |y_i - \\hat{y}_i|^2 . $$\n",
    "\n",
    "When $\\hat{y}$ is the output of a neural network, there are hidden parameters $\\omega_1, \\ldots, \\omega_P$ such that\n",
    "\n",
    "$$ \\hat{y}_i = \\hat{y}_i (\\omega_1, \\ldots, \\omega_P). $$\n",
    "\n",
    "As such we can reinterpret the loss function in terms of these parameters\n",
    "\n",
    "$$ \\widetilde{L} (y, \\omega_1, \\ldots, \\omega_P) = L(y,\\hat{y})$$.\n",
    "\n",
    "The goal is then to minimise $\\widetilde{L}$ with respect to $\\omega_1, \\ldots, \\omega_P$, as these are simply model parameters which we are free to choose.\n",
    "\n",
    "Calculus shows that if $\\widetilde{L}$ is not minimised for some choice of $\\omega_1, \\ldots, \\omega_P$, then the best update for these parameters to minimize $\\widetilde{L}$ is to use gradient descent\n",
    "\n",
    "$$ \\omega_i \\rightarrow \\omega_i - \\frac{\\partial \\widetilde{L}}{\\partial \\omega_i} $$\n",
    "\n",
    "This can overshoot the desired update, so an additional parameter is added, the learning rate $\\gamma>0$ so that\n",
    "\n",
    "$$ \\omega_i \\rightarrow \\omega_i - \\gamma \\frac{\\partial \\widetilde{L}}{\\partial \\omega_i}. $$\n",
    "\n",
    "In fact, the learning rate can be chosen more specifically, for example it could depend on the number of updates (iterations) that have occured.\n",
    "\n",
    "###### Chain Rule\n",
    "\n",
    "Recall that in a neural network\n",
    "\n",
    "$$ \\hat{y} = f_\\lambda \\circ f_{\\lambda-1} \\circ \\ldots f_1 (x). $$\n",
    "\n",
    "Let $\\omega_{i,j}$ denote the parameters of $f_{j}$, that is $f_j$ has parameters $\\omega_{1,j}, \\omega_{2,j}, \\ldots, \\omega_{P_j, j}$.\n",
    "\n",
    "When there are many layers, $\\lambda$ is large, it would be difficult to compute the partial derivatives\n",
    "\n",
    "$$ \\frac{\\partial \\widetilde{L}}{\\partial \\omega_{i,j}} $$ \n",
    "\n",
    "for most layers, besides the final layer $j=\\lambda$. The chain rule helps make this computation simpler, since $\\hat{y}$ is simply a sequence of compositions. The following holds. Let $g_{k}$ denote the following\n",
    "\n",
    "$$ g_k = f_k \\circ f_{k-1} \\circ \\ldots f_1 . $$\n",
    "\n",
    "Then\n",
    "\n",
    "$$ \\widetilde{L}(y, \\omega_{i,j}) = L(y, \\hat{y}) = L(y, f_\\lambda(g_{\\lambda-1} (x)) $$\n",
    "\n",
    "Then \n",
    "\n",
    "$$ \\frac{\\partial \\widetilde{L}}{\\partial \\omega_{i,j}} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial f_\\lambda \\circ g_{\\lambda-1}}{\\partial \\omega_{i,j}}. $$\n",
    "\n",
    "By the chain rule, if $j < \\lambda$ we have that\n",
    "\n",
    "$$ \\frac{ \\partial \\widetilde{L}}{\\partial \\omega_{i,j}} = D_{\\hat{y}} L Df_{\\lambda} \\frac{\\partial g_{\\lambda -1}}{\\partial \\omega_{i,j}}. $$\n",
    "\n",
    "By induction it follows that\n",
    "\n",
    "$$ \\frac{\\partial \\widetilde{L}}{\\partial \\omega_{i,j}} = D_{\\hat{y}} L \\Pi_{k=\\lambda} ^{j} Df_{k}  \\frac{\\partial f_j}{\\partial \\omega_{i,j}}. $$\n",
    "\n",
    "The point of this is that you simply need the derivatives of each $f_j$ with respect to its parameters and inputs to compute the derivative of the loss with respect to the parameters.\n",
    "\n",
    "As such the computation is well suited for looping over. This is called backpropagation. \n",
    "\n",
    "##### Backpropagation\n",
    "\n",
    "Backpropagation is the way the parameters in a neural network are updated. After comparing the predictions $\\hat{y}_1, \\ldots, \\hat{y}_N$ to the true values $y_1, \\ldots, y_N$ via a loss function $L(y, \\hat{y})$, the gradients of $L$ with respect to the parameters of the network are computed by \"backpropagating\" derivatives of the layers in the network.\n",
    "\n",
    "By computing the derivatives of $L$ with respect to the parameters in the last layer, it becomes an inductive exercise to compute the necessary derivatives for all other parameters. At the end of this induction, the parameters can be updated. The key thing is that each layer must know it's derivatives of outputs with respect to inputs, where inputs here includes the parameters for that layer.\n",
    "\n",
    "For a fully connected layer this is easy:\n",
    "\n",
    "$$ y = f_{W,b} (x) \\Rightarrow y_i = W_{ij} x_j + b_i. $$\n",
    "\n",
    "As such\n",
    "\n",
    "$$ \\frac{\\partial y_i}{\\partial x_j} = W_{ij}, \\quad \\frac{\\partial y_i}{\\partial W_{jk}} = \\delta_{ij} x_k, \\quad \\frac{\\partial y_i}{\\partial b_j} = \\delta_{ij}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
